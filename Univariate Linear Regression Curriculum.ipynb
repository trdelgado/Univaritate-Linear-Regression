{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Univariate Linear Regression Curriculum.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"5IgP8I-1zZnn","colab_type":"text"},"source":["**Note: Please make your own copy of this notebook to run and execute, thank you!**\n","\n","1.   Go to the menu tab on the top left corner\n","2.   Click on \"File\"\n","3.   Under the File tab menu click on \"Save a copy in Drive...\""]},{"cell_type":"markdown","metadata":{"id":"sENZFb6ZZyIf","colab_type":"text"},"source":["# Introduction to Univariate Linear Regression"]},{"cell_type":"markdown","metadata":{"id":"NTHLDQpEgU1J","colab_type":"text"},"source":["Suppose we wish to analyze the relationship between a vehicle's weight and fuel economy or the price of a slice of pizza based on the volume of pizza produced. How might we analyze the relationship between these variables? In this lesson, we'll cover univariate linear regression, which is a statistical approach to find and determine a relationship among an independent variable $x$ and a dependent variable $y$."]},{"cell_type":"markdown","metadata":{"id":"lMpVYa7N-S96","colab_type":"text"},"source":["So how does univariate linear regression help us find answers to these questions? As humans, we can plot our data and perhaps observe a general pattern or trend. Univariate linear regression works in a similar way, where it tries to find the best line that fits our data, often called the \"line of best fit\". \n","\n","We'll cover how univariate linear regression is used, how it makes predictions from a linear line equation, uses data to find our best fit line, and finally how to program it from scratch."]},{"cell_type":"markdown","metadata":{"id":"k16WwU9fsC0r","colab_type":"text"},"source":["[Univariate Linear Regression Glossary](https://docs.google.com/document/d/10SHSjLqaU__uw-k3q4iFji70ScCsD0CNANujoEuTCLg/edit):\n","\n","Provided as a list of terms and defintions used in the Univariate Linear Regression lesson to help keep track of the content."]},{"cell_type":"markdown","metadata":{"id":"ZVQOK0flZ2Wy","colab_type":"text"},"source":["# Example of Univariate Linar Regression - Saving to Buy a Phone"]},{"cell_type":"markdown","metadata":{"id":"s2fbLAmU9IOR","colab_type":"text"},"source":["Suppose we currently have 200 dollars saved up and we wish to buy a used phone for 395 dollars. How long would it take for us to pay off this phone if we work as a part-time server for 12 hours per week when part of our income comes from tips? Unfortunately, we do not have a concrete answer since our saving depends on a lot of variables outside of our direct control. However, we can look at our prior income and savings to get an idea of how much money we will be saving in the future.\n"]},{"cell_type":"markdown","metadata":{"id":"6DLyzbpeNJrP","colab_type":"text"},"source":["Assuming our work schedule and phone price are fixed and take into account living expenses and taxes, we can plot our hourly income to discover we save roughly 3 dollars an hour. That is we keep 3 dollars on average for each hour we worked."]},{"cell_type":"markdown","metadata":{"id":"kF3qGRQHNLiX","colab_type":"text"},"source":["Using this information we can estimate that if we would work roughly 65 hours (a little over 5 weeks) at our current saving rate we would be able to make about 195 dollars which we can add to the rest of the money we saved to pay the full price for our phone."]},{"cell_type":"markdown","metadata":{"id":"8YDJlKlj7L-S","colab_type":"text"},"source":["**Questions:**\n","*   If we currently have 250 dollars saved, how many hours would be need to work to pay the phone in full?\n","*   Assume we had 50 dollars saved, but our weekly schedule was 40 hrs a week, how long will it take us to make the full payment?"]},{"cell_type":"markdown","metadata":{"id":"N3L10w9qNrSx","colab_type":"text"},"source":["# Making Predictions with Univariate Linear Regression"]},{"cell_type":"markdown","metadata":{"id":"w3yEX24yzd4r","colab_type":"text"},"source":["Now that we have a concrete idea of how univariate regression can be used, let's convert it into a generalizable formula which we can use to make predictions. In univariate linear regression, we can write the relationship between the independent variable and the dependent variable as $y = mx + b$ where $m$ is the slope and $b$ is our intercept."]},{"cell_type":"markdown","metadata":{"id":"cKDqsb4lNs0e","colab_type":"text"},"source":["[Introduction to Simple Linear Regression](https://www.youtube.com/watch?v=owI7zxCqNY0):\n","\n","In statistics we usually do not know the true relationship between $x$ and $y$ so we use the common notation $\\hat{y} = f(x) = \\beta_1\\ x + \\beta_0$ to estimate and model this relationship with our data. The $\\hat{y}$ represents our predicted or estimated value of $y$ and the $\\beta$ symbols (know as parameters) are basically the same as our slope and intercept notation. To get a better idea of what each term means and how they are calculated check out the first 7:35 minutes of the above video (we will discuss errors and how to fit the model later in this lesson) offered by the INCAE international business school."]},{"cell_type":"markdown","metadata":{"id":"VWu1fN0Nykk0","colab_type":"text"},"source":["In machine learning we typically call $\\hat{y}$ our prediction, $x$ our feature, and parameters $w$ weights. Now let's rewrite our regression equation in machine learning notation."]},{"cell_type":"markdown","metadata":{"id":"aSgymVl6dLry","colab_type":"text"},"source":["**Regression Line:**\n","\n","$$y\\ estimate: \\hat{y}$$\n","\n","$$ feature: x$$\n","\n","$$slope\\ estimate\\ (weight\\ 1): w_1$$\n","\n","$$intercept\\ estimate\\ (weight\\ 0): w_0$$\n","\n","$$\\hat{y} = f(x) = w_1\\ x + w_0$$"]},{"cell_type":"markdown","metadata":{"id":"5Tghd3egPjh3","colab_type":"text"},"source":["To make the above equation more concrete let's briefly go back to our phone example earlier. In this case if we were to replace the theoretical equation with data we gathered from our bank records we can replace the money we have already saved with $w_0$, the saving rate with $w_1$, the hours we plan to work as $x$, and the money we are hoping to save as the $y\\ estimate \\ \\hat{y}$."]},{"cell_type":"markdown","metadata":{"id":"pyjao7Kaci2O","colab_type":"text"},"source":["Now, let's step through an example of how to calculate the estimated $\\hat{y}$ output of a regression where the linear equation is represented by $\\hat{y} = 2x$ and our input feature is $x = 4$. In this case there is no intercept. Instead we just have a slope or $w_1$ of 2 which means for every increment we increase our input it will double the expected output.\n","\n","**Example:**\n","\n","$$w_1 = 2,\\ x = 4,\\ w_0 = 0$$\n","\n","$$\\hat{y} = w_1(x) + w_0 = 2 (x) + 0$$\n","\n","$$\\hat{y} = 2(4) + 0 = 8 + 0 = 8$$"]},{"cell_type":"markdown","metadata":{"id":"4OT0-utsUXGX","colab_type":"text"},"source":["Now that we have an idea of how to calculate a univariate linear regression to make a predicition, let's go ahead and convert this equation into a small function in Python so in the future we can have the computer do all the heavy lifting for us."]},{"cell_type":"code","metadata":{"id":"gRV3K02tcuEs","colab_type":"code","colab":{}},"source":["# Build custom univariate linear regression predictive function\n","def regression(slope, x, intercept):\n","  return slope * x + intercept"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0n3OeotkVRD7","colab_type":"text"},"source":["Now let's check that our regression line works as predicted."]},{"cell_type":"code","metadata":{"id":"QlHZU1cbcuq-","colab_type":"code","colab":{}},"source":["# Supply univariate linear regression model with slope, intercept, and input value\n","slope = 2\n","x = 4\n","intercept = 0\n","print(regression(slope, x, intercept))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Oeie5Pt6VYJ","colab_type":"text"},"source":["**Problem Sets:**\n","*   Calculate the predicted y value of a regression line where the slope is 3, the intercept is 0 and the input value is 5\n","*   Calculate the predicted y value of a regression line where the slope is 2, the intercept is -1 and the input value is 1\n","*   Calculate the predicted y value of a regression line where the slope is 3, the intercept is 2 and the input value is 4"]},{"cell_type":"markdown","metadata":{"id":"k6TAsW_rWGcN","colab_type":"text"},"source":["# Fitting a Univariate Linear Regression Line"]},{"cell_type":"markdown","metadata":{"id":"rNNV-56EWKS6","colab_type":"text"},"source":["As you may have noticed, our formulas only works if we know the slope and intercept of our model, but what if we don't have this information? Before we can even find our slope and intercept we need to ask ourselves an even more important question. How can we determine if the model is accurately capturing our data - in other words is our model a good fit?  Once we have a criteria of how to evaluate a model we can then use this information to find an optimal slope and intercept."]},{"cell_type":"markdown","metadata":{"id":"aQ-ZX9NiWh_K","colab_type":"text"},"source":["[Squared Error of Regression Line](https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data/more-on-regression/v/squared-error-of-regression-line):\n","\n","While we can intuitive draw lines through data we'll need to instruct our computer how to evaluate our regression. One approach we can try is to ***minimize*** the error generated in our model by asking how far off our prediction is from the actual outcome. We can do so by measuring the difference between our predictive model $\\hat{y}$ and the true value $y$ using the sum squared error method what Sal Khan will discuss futher."]},{"cell_type":"markdown","metadata":{"id":"yAOKyc9MW14g","colab_type":"text"},"source":["**Sum of Squared Errors:**\n","\n","$$SSE = \\sum_{i = 0}^{N}(y_i - \\hat{y})^2$$\n"]},{"cell_type":"markdown","metadata":{"id":"_27cHTvTtKy9","colab_type":"text"},"source":["Now that we know how to calculate the sum of squared errors, how do we minimize it to find the best fit line for our linear regression? "]},{"cell_type":"markdown","metadata":{"id":"Q19Y6Hf7Nzxn","colab_type":"text"},"source":["Say for example we have a linear equation where the slope is 1 and the intercept is 0. How can we tell if this is a good model? Well we can compare the predicted values of $\\hat{y}$ from the true values y in our recorded dataset. That is for each point in the data set we would subtract the predict value from the true value in the data and sum the total of squared errors."]},{"cell_type":"markdown","metadata":{"id":"KDCnKL-lHBwW","colab_type":"text"},"source":["**Regression Model 1:**\n","\n","$$\\hat{y} = 1(x)$$\n","\n","$$\\hat{y} = \\{1,\\ 2,\\ 3,\\ 4,\\ 5\\},\\ y = \\{2.1,\\ 3.9,\\ 5.8,\\ 7.9,\\ 10.2\\}$$\n","\n","$$SSE = (2.1-1)^2 + (3.9-2)^2 + (5.8-3)^2 + (7.9-4)^2 + (10.2-5)^2 = 54.91$$"]},{"cell_type":"markdown","metadata":{"id":"26bhvP2tHNwH","colab_type":"text"},"source":["This linear equation ends up having a much higher squarer error value then if the linear equation is represented by another model with a slope and intercept of 2 and 0 respectively. In this case if we were to measure the error (otherise know as a residual) we would have a squared error of roughly 0.1099 for our dataset (due to the size of the dataset and some small amount of randomization)."]},{"cell_type":"markdown","metadata":{"id":"fDrH0zcFNywK","colab_type":"text"},"source":["**Regression Model 2:**\n","\n","$$\\hat{y} = 2(x)$$\n","\n","$$\\hat{y} = \\{2,\\ 4,\\ 6,\\ 8,\\ 10\\},\\ y = \\{2.1,\\ 3.9,\\ 5.8,\\ 7.9,\\ 10.2\\}$$\n","\n","$$SSE = (2.1-2)^2 + (3.9-4)^2 + (5.8-6)^2 + (7.9-8)^2 + (10.2-10)^2 = 0.109\\bar{9}$$"]},{"cell_type":"markdown","metadata":{"id":"wOLurknNYpj9","colab_type":"text"},"source":["Let's convert the sum of squared error function into Python code."]},{"cell_type":"code","metadata":{"id":"lbmLRR1cXJcO","colab_type":"code","colab":{}},"source":["# Sum of Squared Error\n","def sum_sq_error(y_test, y_train):\n","  return sum((y_test - y_train)**2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WYjiXm1nYxqs","colab_type":"text"},"source":["Now, let's verify our code with the example we calculated above."]},{"cell_type":"code","metadata":{"id":"jp48DAIlXMld","colab_type":"code","colab":{}},"source":["# Hypthetical predictive values generated by model\n","y_pred = np.array([2, 4, 6, 8, 10])\n","# Labeled data outcomes to compare how well are model is performing (with some amount of random noise)\n","y_test = np.array([2.1, 3.9, 5.8, 7.9, 10.2])\n","\n","# Output the sum of squared errors between the model and the data\n","print(sum_sq_error(y_test, y_pred))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fGv0gZAqXWWv","colab_type":"text"},"source":["Okay so now that we have an error formula how can we use it to try to find our optimal slope and intercept (our weights) that will reduce our errors? Well, we can analyze the behavior of our SSE and see if it can help us determine which points we can use."]},{"cell_type":"markdown","metadata":{"id":"ncFf5JzIU0kk","colab_type":"text"},"source":["For example, say that we are throwing a baseball in the air which we can represent its trajectory using a parabola. How might we be able to find the point at which the baseball is at its highest point before it is released? Initially, the ball starts out in our hand but moves up before falling down again. From this information, we can assume the ball is at its highest when the ball is no longer moving up but has not yet started to fall. In other words, its velocity - the rate of change of the ball's height is changing is zero. By analyzing the ball's trajectory and setting our velocity to zero we can rearrange our formula to find the ball's highest point.\n"]},{"cell_type":"markdown","metadata":{"id":"OeVTE2auX4vj","colab_type":"text"},"source":["In calculus, we call the rate of change a ***[derivative](https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-1-new/ab-2-2/v/calculus-derivatives-1-new-hd-version)*** which Sal Khan will go into in more detail. While we won't go into all the proofs, in a nutshell, instead of manually testing various weight values, we can take the derivative of the sum of square errors to help us find our ideal weight parameters. Note: if you would like to see how these equations are derived please refer to the extra resources section at the end of this lesson."]},{"cell_type":"markdown","metadata":{"id":"e1MPjoP_SOqY","colab_type":"text"},"source":["**Slope and Intercept:**\n","\n","$$slope\\ estimate: w_1 = \\frac{ss_{xy}}{ss_{xx}}$$\n","\n","$$intercept\\ estimate: w_0 = \\bar{y} - w_1\\bar{x}$$"]},{"cell_type":"markdown","metadata":{"id":"P68mgFxoXdpr","colab_type":"text"},"source":["**Sum of Squares Deviation (used to find the slope):**\n","\n","$$ss_{xx} = \\sum_{i=0}^{N}(x_i - \\bar{x})^2$$\n","\n","$$ss_{yy} = \\sum_{i=0}^{N}(y_i - \\bar{y})^2$$\n","\n","$$ss_{xy} = \\sum_{i=0}^{N}(x_i - \\bar{x})(y_i - \\bar{y})$$\n","\n","**Example:**\n","\n","$$X = \\{x_1,\\ x_2,\\ x_3,\\ x_4,\\ x_5\\} = \\{1,\\ 2,\\ 3,\\ 4,\\ 5\\}$$\n","\n","$$\\bar{x} = 3$$\n","\n","$$ss_{xx} = (1-3)^2 + (2-3)^2 + (3-3)^2 + (4-3)^2 + (5-3)^2 = 10$$\n","\n","$$Y = \\{y_1,\\ y_2,\\ y_3,\\ y_4,\\ y_5\\} = \\{2.1,\\ 3.9,\\ 5.8,\\ 7.9,\\ 10.2\\}$$\n","\n","$$\\bar{y} = 5.98$$\n","\n","$$ss_{yy} = (-2.1-5.98)^2 + (3.9-5.98)^2 + (5.8-5.98)^2 + (7.9-5.98)^2 + (10.2-5.98)^2 = 40.9079\\bar{9}$$\n","\n","$$ss_{xy} = (-2)(-3.88) + (-1)(-2.08) + (0)(-0.18) + (1)(1.92) + (2)(4.22) = 20.2$$\n"]},{"cell_type":"markdown","metadata":{"id":"TQJ_bqcfZWpp","colab_type":"text"},"source":["Once again let's go ahead and code these formulas into Python and verify they calculate the examples above."]},{"cell_type":"code","metadata":{"id":"CXiWNWz4XYoC","colab_type":"code","colab":{}},"source":["# Sum of Squared Deviation for single variable x\n","def ssxx(x):\n","  return sum((x-np.mean(x))**2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vTgUYfdaZSiT","colab_type":"code","colab":{}},"source":["# Sum of Squared Deviation for two variables x and y\n","def ssxy(x, y):\n","  xmean = np.mean(x)\n","  ymean = np.mean(y)\n","  return sum((x-xmean)*(y-ymean))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bgreZlUwZkB_","colab_type":"code","colab":{}},"source":["x = [1, 2, 3, 4, 5]\n","y_pred = np.array([2, 4, 6, 8, 10])\n","y_test = np.array([2.1, 3.9, 5.8, 7.9, 10.2])\n","\n","print(ssxx(x))\n","print(ssxx(y_test))\n","print(ssxy(x, y_test))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pvJ7mypc6ASF","colab_type":"text"},"source":["**Reflection:**\n","\n","*   What is the purpose of calculating the sum of squared errors?\n","*   In general how do we go about trying to find the ideal slope and intercept of a univariate linear regression line?"]},{"cell_type":"markdown","metadata":{"id":"QGD-ROvLZmm9","colab_type":"text"},"source":["# Building Our Univariate Linear Regression Model"]},{"cell_type":"markdown","metadata":{"id":"NWbe1BIgZwNl","colab_type":"text"},"source":["Now that we can calculate the slope and intercept formulas, let's create our linear regression model from scratch and make a new prediction."]},{"cell_type":"code","metadata":{"id":"j6SVsZFlZs1I","colab_type":"code","colab":{}},"source":["# Define the univariate linear regression\n","class univariate_linear_regression_model:\n","  # Initialize slope and intercept (weight parameters)\n","  def __init__(self):\n","    self.b = 0.0\n","    self.i = 0.0\n","  \n","  # Sum of Squared Deviation for single variable x\n","  def ssxx(self, x):\n","    return sum((x-np.mean(x))**2)\n","  \n","  # Sum of Squared Deviation for two variables x and y\n","  def ssxy(self, x, y):\n","    xmean = np.mean(x)\n","    ymean = np.mean(y)\n","    return sum((x-xmean)*(y-ymean))\n","    \n","  # Train regression model based of size and shape of the data\n","  def train(self, x, y):\n","    \n","    # Verify the features and labels of the dataset are of the same size\n","    assert(len(x) == len(y)) \n","    \n","    # Calculate the slope\n","    ss_xy = self.ssxy(x, y)\n","    ss_xx = self.ssxx(x)\n","    self.b = ss_xy/ss_xx\n","    \n","    # Calculate the intercept\n","    mux = np.mean(x)\n","    muy = np.mean(y)\n","    self.i = muy - self.b*mux\n","\t\n","  # Return the predicted value based off the feature and weight parameters\n","  def predict(self, x):\n","    predictions = np.zeros(len(x))\n","    for i in range(len(x)):\n","      predictions[i] = self.b * x[i] + self.i\n","    return predictions"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xQoq0lw8Zr1G","colab_type":"text"},"source":["Alright, let's run the model on some toy data and see if our predictions match our data."]},{"cell_type":"code","metadata":{"id":"no-BKSFQZ3xY","colab_type":"code","colab":{}},"source":["# Dataset to train model\n","x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n","y = np.array([1.1, 1.9, 2.8, 4, 5.2, 5.8, 6.9, 8.1, 9, 9.9])\n","\n","# Initialize our model\n","reg = univariate_linear_regression_model()\n","\n","# Train our model with the data\n","reg.train(x, y)\n","\n","# Make a prediction\n","print(reg.predict(3))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ojb4a2mf5Bzz","colab_type":"text"},"source":["**Reflection:**\n","\n","*   What might happen if we tried to fit our univariate linear regression model on a dataset that was nonlinear (for example an exponential relationship)?\n","*   What applications do you think univariate linear regression might be useful for, what do you think are some of its limitations?"]},{"cell_type":"markdown","metadata":{"id":"MYba0roXaX7M","colab_type":"text"},"source":["# Evaluating Our Univariate Linear Regression"]},{"cell_type":"markdown","metadata":{"id":"q0bb4KLYafkS","colab_type":"text"},"source":["Now that we ran a basic regression model how do we know how effective it is? Earlier we talked about how SSE measures the overall error generated by our model. However, this is not an optimal metric to measure performance since the SSE value is highly dependent on the size of the dataset without giving us a measurement of its effectiveness. It also, unfortunately, tells us nothing about how strong the correlation is between our independent and dependent variables."]},{"cell_type":"markdown","metadata":{"id":"Bd83Tsl7YGc1","colab_type":"text"},"source":["[Coefficient of Correlation](https://www.khanacademy.org/math/ap-statistics/bivariate-data-ap/correlation-coefficient-r/v/calculating-correlation-coefficient-r):\n","\n","One metric to measure this is by using the coefficient of correlation. It is a measure of the strength of the linear relationship between two variables x and y where -1 is a negative correlation, 0 indicates no correlation, and 1 is a strong positive correlation between both variables. The $r$ value is calculated by summing the product differences between a point and its $x$ and $y$ values (commonly known in statistics as [ Z-Scores](https://www.khanacademy.org/math/ap-statistics/density-curves-normal-distribution-ap/measuring-position/v/z-score-introduction)) which Sal will discuss in more detail in the video provided."]},{"cell_type":"markdown","metadata":{"id":"LxBA1nCXah2-","colab_type":"text"},"source":["**Coefficient of Correlation:**\n","\n","$$r = \\frac{1}{n-1}\\sum_{i = 0}^{N}{Z_{x_i}}{Z_{y_i}} =\\frac{1}{n-1}\\sum_{i = 0}^{N}(\\frac{x_i - \\bar{x}}{s_x})(\\frac{y_i - \\bar{y}}{s_y}) = \\frac{ss_{xy}}{\\sqrt{ss_{xx}ss_{yy}}}$$\n","\n","**Example:**\n","\n","$$\\bar{x} = 3,\\ \\bar{y} = 5.98$$\n","\n","$$ss_{xx} = 10,\\ ss_{yy} = 40.908,\\ ss_{xy} = 20.2$$\n","\n","$$r = \\frac{20.2}{\\sqrt{(10)(40.908)}} = 0.998728$$"]},{"cell_type":"markdown","metadata":{"id":"FQAifoLUYXw-","colab_type":"text"},"source":["Now let's once again convert this into code."]},{"cell_type":"code","metadata":{"id":"kPKCJ97kakRT","colab_type":"code","colab":{}},"source":["def coefficient_correlation(x,y):\n","\tss_xy = ssxy(x, y)\n","\tss_xx = ssxx(x)\n","\tss_yy = ssxx(y)\n","\tr = (1.0 * ss_xy)/np.sqrt(ss_xx*ss_yy)\n","\treturn r"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2TsPHdPlamAs","colab_type":"code","colab":{}},"source":["x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n","y = np.array([1.1, 1.9, 2.8, 4, 5.2, 5.8, 6.9, 8.1, 9, 9.9])\n","print(coefficient_correlation(x,y))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_OeQnS8Tanun","colab_type":"text"},"source":["[R-squared or coefficient of determination](https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data/more-on-regression/v/r-squared-or-coefficient-of-determination):\n","\n","Alright, we can now determine a positive or negative correlation between both the independent and dependent values, but can we measure the strength of the independent value on the dependent value? In order to determine how reliable our regression model is we can use another metric called the coefficient of determination represented by the symbol $r^2$. The $r^2$ looks at the explained sample variability from the total sample variability in our data to determine how much our model actually predicts or describes the underlying data."]},{"cell_type":"markdown","metadata":{"id":"gLfCkn3fappT","colab_type":"text"},"source":["**Coefficient of Determination:**\n","\n","$$r^2 = \\frac{ss_{yy} - sse}{ss_{yy}} = 1 - \\frac{sse}{ss_{yy}}$$\n","\n","**Example**:\n","\n","$$ss_{yy} = 40.908,\\ sse = 0.109\\bar{9}$$\n","\n","$$r^2 = 1 - \\frac{0.109\\bar{9}}{40.908} = 0.997311$$"]},{"cell_type":"markdown","metadata":{"id":"UFRpyESWZfO3","colab_type":"text"},"source":["In otherwords our model explains roughly 99.7% of our variance. Now let's convert our metric into code we can later use."]},{"cell_type":"code","metadata":{"id":"fe1GpUWDarRz","colab_type":"code","colab":{}},"source":["def coefficient_determination(y_test, y_train):\n","  ss_yy = ssxx(y_test)\n","  sse = sum_sq_error(y_test, y_train)\n","  r2 = 1 - (1.0*sse/ss_yy)\n","  return r2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hhri3SlDatLW","colab_type":"code","colab":{}},"source":["y_pred = np.array([2, 4, 6, 8, 10])\n","y_test = np.array([2.1, 3.9, 5.8, 7.9, 10.2])\n","print(coefficient_determination(y_test, y_pred))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z5Iuhs8QavwX","colab_type":"text"},"source":["We can use the $r^2$ metric to help us evaluate models during training. In fact, sklearn has its own internal $r^2$ method."]},{"cell_type":"code","metadata":{"id":"mLL60SShaza6","colab_type":"code","colab":{}},"source":["print(r2_score(y_test, y_pred))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qf-AT0No4FB5","colab_type":"text"},"source":["**Reflection:**\n","\n","*   What is the difference between a positive and negative correlation in a dataset? How would this be reflected in our r values?\n","*   If the r squared value of a model was close to zero what would this imply? Is this a model you would use? Why, why not?\n","*   What might be a reason why we might not get a perfect fit between the observed data and the best fit line?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fujOt-gPFEYo","colab_type":"text"},"source":["# Summary"]},{"cell_type":"markdown","metadata":{"id":"o1TTgElJ24ZH","colab_type":"text"},"source":["In this lesson, we covered how univariate linear regression is used to find the relationship between an independent variable x and the dependent variable y. We can gather observed data from linear regression to find an equation that minimizes the error between observed data and a hypothetical best fit line. To measure how accurate our regression line is we can use the R Squared metric which determines how much of the explained variance in the data is described by the model. Assuming we have a good fit we can then use the model to help us make informed predictions on future unseen observations."]},{"cell_type":"markdown","metadata":{"id":"vpQJYYE3X_-f","colab_type":"text"},"source":["# Extra Resources"]},{"cell_type":"markdown","metadata":{"id":"42cYNfeWYZR3","colab_type":"text"},"source":["For those that have some familiarity with calculus and would like to learn more about how to find the optimal weight parameters for univariate linear regression, check out the following several videos where Sal Khan explains them in more detail:\n","\n","\n","*   [Proof (part 1) minimizing squared error to regression line](https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data/more-on-regression/v/proof-part-1-minimizing-squared-error-to-regression-line)\n","*   [Proof (part 2) minimizing squared error to regression line](https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data/more-on-regression/v/proof-part-2-minimizing-squared-error-to-line)\n","*   [Proof (part 3) minimizing squared error to regression line](hhttps://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data/more-on-regression/v/proof-part-3-minimizing-squared-error-to-regression-line)\n","*   [Proof (part 4) minimizing squared error to regression line](https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data/more-on-regression/v/proof-part-4-minimizing-squared-error-to-regression-line)"]}]}